{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "Aquest tutorial demostra com generar text utilitzant una RNN basada en caràcters. Treballaràs amb un conjunt de dades d'escriptura de Shakespeare. Donada una seqüència de caràcters d'aquestes dades (\"Shakespear\"), entrenarem un model per predir el següent caràcter de la seqüència (\"e\"). Es poden generar seqüències de text més llargues cridant el model repetidament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcygKkEVZBaa"
   },
   "source": [
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Descarrega el *dataset*\n",
    "\n",
    "El *dataset* el podem trobar a [l'enllaç](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pD_55cOxLkAb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-09 15:49:37--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.200.91, 216.58.215.187, 142.250.184.187, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.200.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt.1’\n",
      "\n",
      "shakespeare.txt.1   100%[===================>]   1.06M  1.82MB/s    in 0.6s    \n",
      "\n",
      "2025-12-09 15:49:38 (1.82 MB/s) - ‘shakespeare.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Llegim les dades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del text: 1115394 caràcters\n"
     ]
    }
   ],
   "source": [
    "text = open(\"shakespeare.txt\", 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(f'Longitud del text: {len(text)} caràcters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 caràcters únics\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} caràcters únics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Processament del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorització del text\n",
    "\n",
    "Abans de l'entrenament, cal convertir les cadenes a una representació numèrica. A PyTorch, sovint creem diccionaris senzills per *mapejar* caràcters a índexs i viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "a86OoYtO01go"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulari: 65\n",
      "Mostra mapeig: 'a' -> 39\n"
     ]
    }
   ],
   "source": [
    "# Mapeig de caràcters a enters\n",
    "char2idx = # TODO\n",
    "# Mapeig d'enters a caràcters\n",
    "idx2char = # TODO \n",
    "\n",
    "# Convertim tot el text a enters\n",
    "text_as_int = # TODO\n",
    "\n",
    "print(f\"Vocabulari: {len(vocab)}\")\n",
    "print(f\"Mostra mapeig: 'a' -> {char2idx['a']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WLv5Q_2TC2pc"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text_as_int, seq_length):\n",
    "        self.text_as_int = text_as_int\n",
    "        self.seq_length = seq_length\n",
    "        self.total_sequences = len(text_as_int) // (seq_length + 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Índex d'inici per a la seqüència actual\n",
    "        start = idx * (self.seq_length + 1)\n",
    "        # Agafem seq_length + 1 caràcters\n",
    "        chunk = self.text_as_int[start : start + self.seq_length + 1]\n",
    "        \n",
    "        # Entrada: caràcters 0 fins al penúltim\n",
    "        input_seq = # TODO\n",
    "        # Objectiu: caràcters 1 fins l'últim\n",
    "        target_seq = # TODO\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Crear el dataset i el dataloader\n",
    "dataset = ShakespeareDataset(text_as_int, seq_length)\n",
    "BATCH_SIZE = 64\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Embedding`\n",
    "\n",
    "Explicació a partir d'[Stackoverflow](https://stackoverflow.com/questions/75646273/what-is-the-difference-nn-embedding-and-nn-linear) i emprant Gemini.\n",
    "\n",
    "\n",
    "Els models de xarxes neuronals operen mitjançant operacions matemàtiques sobre nombres reals. No obstant això, el text és intrínsecament discret (caràcters o paraules). Tradicionalment, s'utilitzava la codificació **One-Hot**, on cada element del vocabulari es representa com un vector de zeros amb un únic '1' en la posició corresponent al seu índex.\n",
    "\n",
    "Aquesta aproximació presenta dos problemes crítics:\n",
    "* Ineficiència Dimensional: Per a un vocabulari de mida $∣V∣$, es requereixen vectors de dimensió $∣V∣$. En vocabularis grans (ex: 50.000 paraules), això genera vectors extremadament *esparsos* i computacionalment costosos.\n",
    "\n",
    "* Manca de Semàntica: En una representació **One-Hot**, tots els vectors són ortogonals entre si (el producte escalar és 0). Això impedeix que el model capturi relacions de similitud; per a la xarxa, \"ca\" i \"moix\" són tan diferents com \"ca\" i \"taula\".\n",
    "\n",
    "La capa __nn.Embedding__ resol aquests problemes projectant els índexs discrets en un espai vectorial continu de dimensió reduïda (anomenat $d_{model}$ o `embedding_dim`).\n",
    "\n",
    "La següent infografia resumeix el procés que succeeix cada vegada que passem una lletra a aquesta capa:\n",
    "\n",
    "![](infografia.png)\n",
    "\n",
    "#### Explicació pas a pas:\n",
    "\n",
    "1.  **L'Entrada (Índex Discret):**\n",
    "    * El model rep un nombre enter (per exemple, l'índex `3` que representa la lletra `'d'`). \n",
    "    * Aquest nombre no té cap significat matemàtic per si sol (el 3 no és \"més\" que el 2).\n",
    "\n",
    "2.  **La \"Lookup Table\" (La Matriu $E$):**\n",
    "    * La capa d'Embedding és, en essència, una matriu de dimensions `(vocab_size, embedding_dim)`.\n",
    "    * En el nostre cas, és una matriu de `65 x 256`.\n",
    "    * **Important:** Aquesta matriu conté **pesos entrenables**. Al principi són aleatoris, però la xarxa els anirà modificant (aprenent) durant l'entrenament.\n",
    "\n",
    "3.  **L'Operació (Selecció):**\n",
    "    * En lloc de fer càlculs complexos, la xarxa utilitza l'índex d'entrada per **seleccionar i extreure** directament la fila corresponent.\n",
    "    * Si l'entrada és `3`, copiem la fila 3.\n",
    "\n",
    "4.  **La Sortida (Vector Dens):**\n",
    "    * El resultat és un vector de nombres decimals (mida 256) que ara sí que conté informació \"rica\" sobre el caràcter.\n",
    "    * Aquest vector és el que alimentem a la capa recurrent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "c2GCh0ySD44s"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # batch_first=True perquè les nostres entrades tenen forma (batch, seq, feature)\n",
    "        self.rnn = nn.RNN(embedding_dim, rnn_units, batch_first=True)\n",
    "        \n",
    "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output shape: (batch_size, seq_length, rnn_units)\n",
    "        # hidden shape: (1, batch_size, rnn_units) -> estat final\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Passem la sortida per la capa densa per obtenir logits\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Inicialitza l'estat ocult a zero\n",
    "        return torch.zeros(1, batch_size, rnn_units).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FeW5gqutT3o"
   },
   "source": [
    "Instancia el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxYI-PeltqKP"
   },
   "outputs": [],
   "source": [
    "# Paràmetres del model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, rnn_units).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaluació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agafem un batch del dataloader\n",
    "input_example_batch, target_example_batch = next(iter(dataloader))\n",
    "input_example_batch = input_example_batch.to(device)\n",
    "\n",
    "# Pas endavant inicial\n",
    "example_batch_predictions, _ = model(input_example_batch)\n",
    "print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: \n",
      " ls true, 'tis there,\n",
      "That, like an eagle in a dove-cote, I\n",
      "Flutter'd your Volscians in Corioli:\n",
      "Alon\n",
      "\n",
      "Prediccions del següent caràcter: \n",
      " YBjbm LHL.lwfolSyXDbT$vTk!BgTWDQuRoy3Cmz 'K'pp!zeRpzxgfFLL-BPXW&. xRQTCx$sJ.QkHElRcsycoRAy?eE?-3B.uy\n"
     ]
    }
   ],
   "source": [
    "# Provem-ho per al primer exemple del batch\n",
    "sampled_indices = torch.distributions.Categorical(logits=example_batch_predictions[0]).sample()\n",
    "sampled_indices = sampled_indices.cpu().numpy()\n",
    "\n",
    "print(\"Entrada: \\n\", \"\".join(idx2char[input_example_batch[0].cpu().numpy()]))\n",
    "print()\n",
    "print(\"Prediccions del següent caràcter: \\n\", \"\".join(idx2char[sampled_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = # TODO\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Època 1 Lot 0 Pèrdua 4.1844\n",
      "Època 1 Lot 50 Pèrdua 2.0527\n",
      "Època 1 Lot 100 Pèrdua 1.9310\n",
      "Època 1 Lot 150 Pèrdua 1.7340\n",
      "Temps per a l'època 1: 5.49 seg\n",
      "Pèrdua mitjana de l'època: 2.0146\n",
      "________________________________________________________________________________\n",
      "Època 2 Lot 0 Pèrdua 1.7000\n",
      "Època 2 Lot 50 Pèrdua 1.6490\n",
      "Època 2 Lot 100 Pèrdua 1.6036\n",
      "Època 2 Lot 150 Pèrdua 1.5357\n",
      "Temps per a l'època 2: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.6189\n",
      "________________________________________________________________________________\n",
      "Època 3 Lot 0 Pèrdua 1.5047\n",
      "Època 3 Lot 50 Pèrdua 1.5232\n",
      "Època 3 Lot 100 Pèrdua 1.5288\n",
      "Època 3 Lot 150 Pèrdua 1.4592\n",
      "Temps per a l'època 3: 5.53 seg\n",
      "Pèrdua mitjana de l'època: 1.5029\n",
      "________________________________________________________________________________\n",
      "Època 4 Lot 0 Pèrdua 1.4392\n",
      "Època 4 Lot 50 Pèrdua 1.4595\n",
      "Època 4 Lot 100 Pèrdua 1.4556\n",
      "Època 4 Lot 150 Pèrdua 1.4190\n",
      "Temps per a l'època 4: 5.50 seg\n",
      "Pèrdua mitjana de l'època: 1.4409\n",
      "________________________________________________________________________________\n",
      "Època 5 Lot 0 Pèrdua 1.3587\n",
      "Època 5 Lot 50 Pèrdua 1.4062\n",
      "Època 5 Lot 100 Pèrdua 1.4092\n",
      "Època 5 Lot 150 Pèrdua 1.4020\n",
      "Temps per a l'època 5: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.3988\n",
      "________________________________________________________________________________\n",
      "Època 6 Lot 0 Pèrdua 1.3014\n",
      "Època 6 Lot 50 Pèrdua 1.3435\n",
      "Època 6 Lot 100 Pèrdua 1.3574\n",
      "Època 6 Lot 150 Pèrdua 1.3561\n",
      "Temps per a l'època 6: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.3666\n",
      "________________________________________________________________________________\n",
      "Època 7 Lot 0 Pèrdua 1.3456\n",
      "Època 7 Lot 50 Pèrdua 1.3662\n",
      "Època 7 Lot 100 Pèrdua 1.3740\n",
      "Època 7 Lot 150 Pèrdua 1.3852\n",
      "Temps per a l'època 7: 5.54 seg\n",
      "Pèrdua mitjana de l'època: 1.3409\n",
      "________________________________________________________________________________\n",
      "Època 8 Lot 0 Pèrdua 1.2975\n",
      "Època 8 Lot 50 Pèrdua 1.3464\n",
      "Època 8 Lot 100 Pèrdua 1.3182\n",
      "Època 8 Lot 150 Pèrdua 1.3276\n",
      "Temps per a l'època 8: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.3178\n",
      "________________________________________________________________________________\n",
      "Època 9 Lot 0 Pèrdua 1.3207\n",
      "Època 9 Lot 50 Pèrdua 1.2752\n",
      "Època 9 Lot 100 Pèrdua 1.2876\n",
      "Època 9 Lot 150 Pèrdua 1.3087\n",
      "Temps per a l'època 9: 5.51 seg\n",
      "Pèrdua mitjana de l'època: 1.2993\n",
      "________________________________________________________________________________\n",
      "Època 10 Lot 0 Pèrdua 1.2538\n",
      "Època 10 Lot 50 Pèrdua 1.2626\n",
      "Època 10 Lot 100 Pèrdua 1.2851\n",
      "Època 10 Lot 150 Pèrdua 1.3348\n",
      "Temps per a l'època 10: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.2799\n",
      "________________________________________________________________________________\n",
      "Època 11 Lot 0 Pèrdua 1.2647\n",
      "Època 11 Lot 50 Pèrdua 1.2781\n",
      "Època 11 Lot 100 Pèrdua 1.2412\n",
      "Època 11 Lot 150 Pèrdua 1.3074\n",
      "Temps per a l'època 11: 5.56 seg\n",
      "Pèrdua mitjana de l'època: 1.2638\n",
      "________________________________________________________________________________\n",
      "Època 12 Lot 0 Pèrdua 1.2118\n",
      "Època 12 Lot 50 Pèrdua 1.2431\n",
      "Època 12 Lot 100 Pèrdua 1.2315\n",
      "Època 12 Lot 150 Pèrdua 1.2778\n",
      "Temps per a l'època 12: 5.55 seg\n",
      "Pèrdua mitjana de l'època: 1.2480\n",
      "________________________________________________________________________________\n",
      "Època 13 Lot 0 Pèrdua 1.2222\n",
      "Època 13 Lot 50 Pèrdua 1.2110\n",
      "Època 13 Lot 100 Pèrdua 1.2348\n",
      "Època 13 Lot 150 Pèrdua 1.2418\n",
      "Temps per a l'època 13: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.2320\n",
      "________________________________________________________________________________\n",
      "Època 14 Lot 0 Pèrdua 1.2015\n",
      "Època 14 Lot 50 Pèrdua 1.1996\n",
      "Època 14 Lot 100 Pèrdua 1.2132\n",
      "Època 14 Lot 150 Pèrdua 1.2341\n",
      "Temps per a l'època 14: 5.48 seg\n",
      "Pèrdua mitjana de l'època: 1.2173\n",
      "________________________________________________________________________________\n",
      "Època 15 Lot 0 Pèrdua 1.1882\n",
      "Època 15 Lot 50 Pèrdua 1.1891\n",
      "Època 15 Lot 100 Pèrdua 1.2510\n",
      "Època 15 Lot 150 Pèrdua 1.2500\n",
      "Temps per a l'època 15: 5.49 seg\n",
      "Pèrdua mitjana de l'època: 1.2046\n",
      "________________________________________________________________________________\n",
      "Època 16 Lot 0 Pèrdua 1.1858\n",
      "Època 16 Lot 50 Pèrdua 1.2337\n",
      "Època 16 Lot 100 Pèrdua 1.1992\n",
      "Època 16 Lot 150 Pèrdua 1.2243\n",
      "Temps per a l'època 16: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.1910\n",
      "________________________________________________________________________________\n",
      "Època 17 Lot 0 Pèrdua 1.1397\n",
      "Època 17 Lot 50 Pèrdua 1.1672\n",
      "Època 17 Lot 100 Pèrdua 1.2213\n",
      "Època 17 Lot 150 Pèrdua 1.1792\n",
      "Temps per a l'època 17: 5.50 seg\n",
      "Pèrdua mitjana de l'època: 1.1760\n",
      "________________________________________________________________________________\n",
      "Època 18 Lot 0 Pèrdua 1.1157\n",
      "Època 18 Lot 50 Pèrdua 1.1198\n",
      "Època 18 Lot 100 Pèrdua 1.1594\n",
      "Època 18 Lot 150 Pèrdua 1.1941\n",
      "Temps per a l'època 18: 5.52 seg\n",
      "Pèrdua mitjana de l'època: 1.1652\n",
      "________________________________________________________________________________\n",
      "Època 19 Lot 0 Pèrdua 1.1433\n",
      "Època 19 Lot 50 Pèrdua 1.1266\n",
      "Època 19 Lot 100 Pèrdua 1.1617\n",
      "Època 19 Lot 150 Pèrdua 1.1634\n",
      "Temps per a l'època 19: 5.53 seg\n",
      "Pèrdua mitjana de l'època: 1.1514\n",
      "________________________________________________________________________________\n",
      "Època 20 Lot 0 Pèrdua 1.1234\n",
      "Època 20 Lot 50 Pèrdua 1.1392\n",
      "Època 20 Lot 100 Pèrdua 1.1739\n",
      "Època 20 Lot 150 Pèrdua 1.1458\n",
      "Temps per a l'època 20: 5.55 seg\n",
      "Pèrdua mitjana de l'època: 1.1395\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20 # Pots augmentar-ho per millors resultats\n",
    "print_every = 50\n",
    "\n",
    "# Historial de pèrdues\n",
    "history = []\n",
    "\n",
    "model.train() # Mode entrenament\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    hidden = None # Reiniciem l'estat ocult a l'inici de cada època (opcional, depèn de l'estratègia)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Reiniciar gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = model(inputs)\n",
    "        \n",
    "        # Flatten per calcular la pèrdua\n",
    "        # Output: (batch * seq_len, vocab_size)\n",
    "        # Targets: (batch * seq_len)\n",
    "        loss = loss_fn(output.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualitzar pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % print_every == 0:\n",
    "            print(f\"Època {epoch+1} Lot {batch_idx} Pèrdua {loss.item():.4f}\")\n",
    "\n",
    "    print(f'Temps per a l\\'època {epoch+1}: {time.time() - start:.2f} seg')\n",
    "    print(f'Pèrdua mitjana de l\\'època: {epoch_loss / len(dataloader):.4f}')\n",
    "    print('_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
    "    # Mode avaluació (important per dropout o batchnorm si n'hi hagués)\n",
    "    model.eval()\n",
    "\n",
    "    # Convertir cadena inicial a números (vectoritzar)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Emmagatzemar el text generat\n",
    "    text_generated = []\n",
    "\n",
    "    # Inicialitzar estat ocult\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_generate):\n",
    "            # Forward pass\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "\n",
    "            # Només ens interessa l'últim pas de temps (l'últim caràcter predit)\n",
    "            predictions = output[:, -1, :] \n",
    "            \n",
    "            # Aplicar temperatura\n",
    "            predictions = predictions / temperature\n",
    "            \n",
    "            # Mostrejar utilitzant una distribució categòrica\n",
    "            dist = torch.distributions.Categorical(logits=predictions)\n",
    "            predicted_id = dist.sample()\n",
    "\n",
    "            # Passar el caràcter predit com a següent entrada\n",
    "            # Afegim una dimensió de batch (1, 1)\n",
    "            input_eval = predicted_id.unsqueeze(0)\n",
    "\n",
    "            # Convertir a caràcter i guardar\n",
    "            text_generated.append(idx2char[predicted_id.item()])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feines a fer\n",
    "\n",
    "1. Completar el codi.\n",
    "2. Provar d'entrenar el model amb el text `tirant.txt` (trobau-lo a la mateixa carpeta de `GitHub`)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
